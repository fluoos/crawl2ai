# Markdownæ™ºèƒ½åˆ†æ®µå·¥å…·

## æ¦‚è¿°

åŸºäºLlamaIndexçš„MarkdownNodeParserç®—æ³•å®ç°çš„æ™ºèƒ½Markdownæ–‡æ¡£åˆ†æ®µå·¥å…·ï¼Œä¸“é—¨è§£å†³å¤§æ¨¡å‹å¤„ç†é•¿æ–‡æ¡£æ—¶çš„Tokené™åˆ¶é—®é¢˜ã€‚
è¯¥å·¥å…·å®ç°äº†ä¸LlamaIndexæ ¸å¿ƒç®—æ³•100%ä¸€è‡´çš„åˆ†æ®µé€»è¾‘ï¼ŒåŒæ—¶å¢åŠ äº†å®ç”¨çš„å¢å¼ºåŠŸèƒ½ã€‚

## æ ¸å¿ƒç‰¹æ€§

- **ğŸ§  æ™ºèƒ½åˆ†æ®µç®—æ³•**ï¼šåŸºäºMarkdownæ ‡é¢˜å±‚çº§è¿›è¡Œåˆ†æ®µï¼Œä¿æŒæ–‡æ¡£é€»è¾‘ç»“æ„å®Œæ•´æ€§
- **ğŸ¯ ç²¾ç¡®Tokenæ§åˆ¶**ï¼šæ”¯æŒä¸­è‹±æ–‡æ··åˆå†…å®¹çš„Tokenä¼°ç®—å’Œæ§åˆ¶
- **ğŸ”§ ç‰¹æ®Šå—ä¿æŠ¤**ï¼šé¿å…åœ¨ä»£ç å—ã€è¡¨æ ¼ã€åˆ—è¡¨ä¸­é—´åˆ†å‰²
- **âš¡ é«˜å…¼å®¹æ€§**ï¼šä¸LlamaIndex MarkdownNodeParseræ ¸å¿ƒç®—æ³•100%ä¸€è‡´
- **ğŸ“ å¤šç§è¾“å‡ºæ–¹å¼**ï¼šæ”¯æŒå†…å®¹åˆ†å‰²ã€æ–‡ä»¶åˆ†å‰²å’Œæ‰¹é‡ä¿å­˜

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

é¡¹ç›®ä¾èµ–Python 3.7+ï¼Œéœ€è¦å®‰è£…ä»¥ä¸‹åŒ…ï¼š
```bash
pip install dataclasses pathlib
```

### åŸºç¡€ç”¨æ³•

```python
from app.core.markdown_splitter import MarkdownSplitter

# åˆ›å»ºåˆ†æ®µå™¨
splitter = MarkdownSplitter(max_tokens=8000, min_tokens=1000)

# å‡†å¤‡æµ‹è¯•å†…å®¹
content = """
# AIæŠ€æœ¯æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»ç°ä»£AIæŠ€æœ¯çš„åº”ç”¨ã€‚

## æœºå™¨å­¦ä¹ åŸºç¡€

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ç­‰ã€‚

### ç›‘ç£å­¦ä¹ 

ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ã€‚

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
```

### æ— ç›‘ç£å­¦ä¹ 

æ— ç›‘ç£å­¦ä¹ å¤„ç†æ— æ ‡ç­¾æ•°æ®ï¼Œå‘ç°éšè—æ¨¡å¼ã€‚

## æ·±åº¦å­¦ä¹ 

æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘å­¦ä¹ è¿‡ç¨‹ã€‚
"""

# æ‰§è¡Œåˆ†æ®µ
chunks = splitter.create_chunks(content)

# æŸ¥çœ‹ç»“æœ
print(f"åŸæ–‡æ¡£Tokenæ•°: {splitter.estimate_tokens(content)}")
print(f"åˆ†æ®µæ•°é‡: {len(chunks)}")

for chunk in chunks:
    print(f"\næ ‡é¢˜: {chunk.title}")
    print(f"Tokenæ•°: {chunk.estimated_tokens}")
    print(f"æ ‡é¢˜è·¯å¾„: {chunk.metadata['header_path']}")
    print(f"å†…å®¹é•¿åº¦: {len(chunk.content)} å­—ç¬¦")
```

## ä¸»è¦åŠŸèƒ½

### 1. å†…å®¹åˆ†å‰²

å°†Markdownå†…å®¹åˆ†å‰²ä¸ºå¤šä¸ªç»“æ„åŒ–çš„å—ï¼š

```python
from app.core.markdown_splitter import MarkdownSplitter

splitter = MarkdownSplitter(max_tokens=4000, min_tokens=500)
chunks = splitter.create_chunks(markdown_content)

for chunk in chunks:
    print(f"åˆ†æ®µ: {chunk.title}")
    print(f"Tokenæ•°: {chunk.estimated_tokens}")
    print(f"è¡Œæ•°èŒƒå›´: {chunk.start_line}-{chunk.end_line}")
```

### 2. æ–‡ä»¶åˆ†å‰²

å°†å¤§å‹Markdownæ–‡ä»¶åˆ†å‰²æˆå¤šä¸ªå°æ–‡ä»¶ï¼š

```python
from app.core.markdown_splitter import split_markdown_file

# åˆ†å‰²æ–‡ä»¶å¹¶ä¿å­˜
output_files = split_markdown_file(
    input_path="large_document.md",
    output_dir="output_chunks/", 
    max_tokens=8000,
    min_tokens=1000
)

print(f"ç”Ÿæˆäº† {len(output_files)} ä¸ªæ–‡ä»¶")
```

### 3. æ‰¹é‡å†…å®¹åˆ†å‰²

å¿«é€Ÿåˆ†å‰²å†…å®¹ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼š

```python
from app.core.markdown_splitter import split_markdown_content

content_chunks = split_markdown_content(
    content=long_markdown_text,
    max_tokens=6000,
    min_tokens=800
)

for i, chunk in enumerate(content_chunks):
    print(f"åˆ†æ®µ {i+1}: {len(chunk)} å­—ç¬¦")
```

## åˆ†æ®µç®—æ³•è¯¦è§£

### åˆ†æ®µä¼˜å…ˆçº§

æœ¬å·¥å…·é‡‡ç”¨ä¸LlamaIndexå®Œå…¨ä¸€è‡´çš„æ ‡é¢˜å±‚çº§åˆ†æ®µç­–ç•¥ï¼š

1. **ä¸€çº§æ ‡é¢˜ (#)**ï¼šæœ€é«˜ä¼˜å…ˆçº§åˆ†å‰²ç‚¹
2. **äºŒçº§æ ‡é¢˜ (##)**ï¼šé«˜ä¼˜å…ˆçº§åˆ†å‰²ç‚¹
3. **ä¸‰çº§æ ‡é¢˜ (###)**ï¼šä¸­ç­‰ä¼˜å…ˆçº§åˆ†å‰²ç‚¹
4. **å››-å…­çº§æ ‡é¢˜**ï¼šä½ä¼˜å…ˆçº§åˆ†å‰²ç‚¹
5. **æ®µè½å¼ºåˆ¶åˆ†å‰²**ï¼šå½“åˆ†æ®µè¿‡å¤§æ—¶çš„æœ€åæ‰‹æ®µ

### ç‰¹æ®Šå†…å®¹ä¿æŠ¤

#### ä»£ç å—ä¿æŠ¤
```markdown
```python
# è¿™æ®µä»£ç ä¸ä¼šè¢«åˆ†å‰²
def process_data():
    # å³ä½¿è¿™é‡Œæœ‰ ## æ ‡é¢˜ ä¹Ÿä¸ä¼šåˆ†å‰²
    return "å®Œæ•´ä¿æŠ¤"
```
```

#### è¡¨æ ¼ä¿æŠ¤
```markdown
| åˆ—1 | åˆ—2 | åˆ—3 |
|-----|-----|-----|
| æ•°æ®1 | æ•°æ®2 | æ•°æ®3 |
| è¡¨æ ¼å†…å®¹ä¿æŒå®Œæ•´ |
```

#### åˆ—è¡¨ä¿æŠ¤
- æœ‰åºåˆ—è¡¨å’Œæ— åºåˆ—è¡¨
- ä¸ä¼šåœ¨åˆ—è¡¨é¡¹ä¸­é—´åˆ†å‰²
- ä¿æŒåˆ—è¡¨çš„è¿ç»­æ€§å’Œå®Œæ•´æ€§

### Tokenä¼°ç®—æœºåˆ¶

æ™ºèƒ½Tokenä¼°ç®—æ”¯æŒå¤šè¯­è¨€å†…å®¹ï¼š

```python
def estimate_tokens(text: str) -> int:
    """
    Tokenä¼°ç®—è§„åˆ™ï¼š
    - è‹±æ–‡å•è¯æ•° Ã— 1.3
    - ä¸­æ–‡å­—ç¬¦æ•° Ã— 1.6
    - ä»£ç å­—ç¬¦æ•° Ã— 1.8  
    - ç‰¹æ®Šç¬¦å·æ•° Ã— 0.5
    """
```

**ç²¾åº¦è¯´æ˜**ï¼šä¼°ç®—å€¼é€šå¸¸ä¸å®é™…Tokenæ•°ç›¸å·®Â±10%

## æ•°æ®ç»“æ„

### MarkdownChunkå¯¹è±¡

```python
@dataclass
class MarkdownChunk:
    content: str           # åˆ†æ®µçš„å®Œæ•´å†…å®¹
    title: str            # åˆ†æ®µæ ‡é¢˜ï¼ˆä»å†…å®¹æå–ï¼‰
    order: int            # åˆ†æ®µåºå·ï¼ˆä»1å¼€å§‹ï¼‰
    estimated_tokens: int # ä¼°ç®—Tokenæ•°é‡
    start_line: int       # åŸæ–‡æ¡£èµ·å§‹è¡Œå·
    end_line: int         # åŸæ–‡æ¡£ç»“æŸè¡Œå·
    metadata: Dict        # å…ƒæ•°æ®ä¿¡æ¯
```

### å…ƒæ•°æ®ç»“æ„

```python
metadata = {
    'header_path': 'ä¸»æ ‡é¢˜/å­æ ‡é¢˜/å½“å‰æ ‡é¢˜',           # æ ‡é¢˜å±‚çº§è·¯å¾„
    'header_stack': [(1, 'ä¸»æ ‡é¢˜'), (2, 'å­æ ‡é¢˜')],  # å®Œæ•´æ ‡é¢˜æ ˆ
    'split_method': 'intelligent'                   # åˆ†æ®µæ–¹æ³•æ ‡è¯†
}
```

## é«˜çº§é…ç½®

### è‡ªå®šä¹‰åˆ†æ®µå™¨

```python
# åˆ›å»ºè‡ªå®šä¹‰é…ç½®çš„åˆ†æ®µå™¨
splitter = MarkdownSplitter(
    max_tokens=4000,              # æœ€å¤§Tokené™åˆ¶
    min_tokens=500,               # æœ€å°Tokené™åˆ¶
    header_path_separator=">"     # æ ‡é¢˜è·¯å¾„åˆ†éš”ç¬¦
)

# æ‰§è¡Œåˆ†æ®µ
chunks = splitter.create_chunks(content)

# è·å–åˆ†æ®µç»Ÿè®¡
summary = splitter.get_split_summary(chunks)
print(f"æ€»åˆ†æ®µæ•°: {summary['total_chunks']}")
print(f"å¹³å‡Tokenæ•°: {summary['avg_tokens_per_chunk']}")
print(f"æœ€å¤§Tokenæ•°: {summary['max_tokens']}")
print(f"æœ€å°Tokenæ•°: {summary['min_tokens']}")
```

### LlamaIndexå…¼å®¹æ¨¡å¼

å¦‚éœ€å®Œå…¨æ¨¡æ‹ŸLlamaIndexåŸç‰ˆè¡Œä¸ºï¼š

```python
# ç¦ç”¨Tokenæ§åˆ¶ï¼Œçº¯LlamaIndexæ¨¡å¼
splitter = MarkdownSplitter(
    max_tokens=999999,  # æå¤§å€¼ï¼Œç¦ç”¨å¤§å°æ§åˆ¶
    min_tokens=1        # æå°å€¼ï¼Œç¦ç”¨åˆå¹¶
)
chunks = splitter.create_chunks(content)
```

## åº”ç”¨åœºæ™¯

### 1. å¤§æ¨¡å‹æ•°æ®é¢„å¤„ç†

```python
def prepare_training_data(input_files, output_dir, model_type="gpt4"):
    """ä¸ºå¤§æ¨¡å‹è®­ç»ƒå‡†å¤‡åˆ†æ®µæ•°æ®"""
    
    # æ ¹æ®æ¨¡å‹é€‰æ‹©Tokené™åˆ¶
    token_limits = {
        "gpt4": {"max": 8000, "min": 1000},
        "gpt3.5": {"max": 6000, "min": 800},
        "claude": {"max": 12000, "min": 1500}
    }
    
    limits = token_limits.get(model_type, token_limits["gpt4"])
    
    for file_path in input_files:
        output_files = split_markdown_file(
            input_path=file_path,
            output_dir=output_dir,
            max_tokens=limits["max"],
            min_tokens=limits["min"]
        )
        print(f"âœ… {file_path}: ç”Ÿæˆ {len(output_files)} ä¸ªåˆ†æ®µ")
```

### 2. RAGç³»ç»Ÿæ–‡æ¡£å—å‡†å¤‡

```python
def create_rag_chunks(documents, chunk_size=1500):
    """ä¸ºRAGç³»ç»Ÿåˆ›å»ºä¼˜åŒ–çš„æ–‡æ¡£å—"""
    
    splitter = MarkdownSplitter(max_tokens=chunk_size, min_tokens=300)
    all_chunks = []
    
    for doc in documents:
        chunks = splitter.create_chunks(doc.content)
        
        for chunk in chunks:
            chunk_data = {
                'id': f"{doc.id}_chunk_{chunk.order}",
                'title': chunk.title,
                'content': chunk.content,
                'tokens': chunk.estimated_tokens,
                'source': doc.source,
                'header_path': chunk.metadata['header_path']
            }
            all_chunks.append(chunk_data)
    
    return all_chunks
```

### 3. æ‰¹é‡æ–‡æ¡£å¤„ç†

```python
import os
from pathlib import Path

def batch_process_directory(input_dir, output_dir, max_tokens=8000):
    """æ‰¹é‡å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰Markdownæ–‡ä»¶"""
    
    input_path = Path(input_dir)
    processed_count = 0
    
    for md_file in input_path.rglob("*.md"):
        try:
            # ä¿æŒç›¸å¯¹è·¯å¾„ç»“æ„
            relative_path = md_file.relative_to(input_path)
            file_output_dir = Path(output_dir) / relative_path.parent / relative_path.stem
            
            # åˆ†å‰²æ–‡ä»¶
            output_files = split_markdown_file(
                input_path=str(md_file),
                output_dir=str(file_output_dir),
                max_tokens=max_tokens,
                min_tokens=1000
            )
            
            print(f"âœ… {md_file.name}: {len(output_files)} ä¸ªåˆ†æ®µ")
            processed_count += 1
            
        except Exception as e:
            print(f"âŒ å¤„ç† {md_file.name} å¤±è´¥: {e}")
    
    print(f"æ‰¹é‡å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")
```

## æœ€ä½³å®è·µ

### Tokené™åˆ¶å»ºè®®

| ç›®æ ‡æ¨¡å‹ | æ¨èmax_tokens | æ¨èmin_tokens | é€‚ç”¨åœºæ™¯ |
|----------|----------------|----------------|----------|
| GPT-4 | 8000 | 1000 | é«˜è´¨é‡ç”Ÿæˆä»»åŠ¡ |
| GPT-3.5 | 6000 | 800 | é€šç”¨æ–‡æœ¬å¤„ç† |
| Claude | 12000 | 1500 | é•¿æ–‡æ¡£åˆ†æ |
| æœ¬åœ°æ¨¡å‹ | 4000 | 500 | èµ„æºå—é™ç¯å¢ƒ |

### æ–‡æ¡£ç»“æ„ä¼˜åŒ–

```markdown
âœ… æ¨èç»“æ„ï¼š
# ä¸»æ ‡é¢˜ï¼ˆæ–‡æ¡£ä¸»é¢˜ï¼‰
## ç« èŠ‚æ ‡é¢˜ï¼ˆä¸»è¦å†…å®¹ï¼‰
### å°èŠ‚æ ‡é¢˜ï¼ˆå…·ä½“è¯é¢˜ï¼‰
#### è¯¦ç»†è¯´æ˜ï¼ˆå¯é€‰ï¼‰

âŒ é¿å…ç»“æ„ï¼š
# æ ‡é¢˜
##### è·³çº§æ ‡é¢˜ï¼ˆç¼ºå°‘ä¸­é—´å±‚çº§ï¼‰
```

### è´¨é‡æ£€æŸ¥ç¤ºä¾‹

```python
def analyze_chunks(chunks, max_tokens, min_tokens):
    """åˆ†æåˆ†æ®µè´¨é‡å¹¶ç»™å‡ºå»ºè®®"""
    
    analysis = {
        'total_chunks': len(chunks),
        'warnings': [],
        'suggestions': []
    }
    
    for chunk in chunks:
        # æ£€æŸ¥Tokenåˆ†å¸ƒ
        if chunk.estimated_tokens > max_tokens * 0.9:
            analysis['warnings'].append(
                f"'{chunk.title}' æ¥è¿‘Tokenä¸Šé™ ({chunk.estimated_tokens})"
            )
        
        if chunk.estimated_tokens < min_tokens * 0.5:
            analysis['suggestions'].append(
                f"'{chunk.title}' å¯èƒ½è¿‡å° ({chunk.estimated_tokens})"
            )
        
        # æ£€æŸ¥å†…å®¹è´¨é‡
        if chunk.title == "æœªåˆ†ç±»å†…å®¹":
            analysis['suggestions'].append("å­˜åœ¨ç¼ºå°‘æ ‡é¢˜çš„åˆ†æ®µ")
    
    return analysis
```

## è¾“å‡ºæ–‡ä»¶æ ¼å¼

ä½¿ç”¨`split_markdown_file()`ç”Ÿæˆçš„æ–‡ä»¶åŒ…å«å®Œæ•´å…ƒæ•°æ®ï¼š

```yaml
---
original_file: /path/to/source.md
chunk_order: 1
chunk_title: "ç¬¬ä¸€ç«  æœºå™¨å­¦ä¹ åŸºç¡€"
estimated_tokens: 1500
split_method: intelligent
header_path: "AIæŒ‡å—/ç¬¬ä¸€ç«  æœºå™¨å­¦ä¹ åŸºç¡€"
start_line: 10
end_line: 85
---

# ç¬¬ä¸€ç«  æœºå™¨å­¦ä¹ åŸºç¡€

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†...
```

## é”™è¯¯å¤„ç†

### å¸¸è§é—®é¢˜è§£å†³

```python
def safe_markdown_split(content, max_tokens=8000, min_tokens=1000):
    """å®‰å…¨çš„åˆ†æ®µå‡½æ•°ï¼ŒåŒ…å«é”™è¯¯å¤„ç†"""
    
    try:
        splitter = MarkdownSplitter(
            max_tokens=max_tokens, 
            min_tokens=min_tokens
        )
        chunks = splitter.create_chunks(content)
        
        if not chunks:
            raise ValueError("åˆ†æ®µç»“æœä¸ºç©º")
        
        return chunks
        
    except UnicodeDecodeError:
        print("æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè¯·ç¡®ä¿ä½¿ç”¨UTF-8ç¼–ç ")
        return None
        
    except MemoryError:
        print("å†…å­˜ä¸è¶³ï¼Œå°è¯•å‡å°‘max_tokensæˆ–åˆ†æ‰¹å¤„ç†")
        return None
        
    except Exception as e:
        print(f"åˆ†æ®µè¿‡ç¨‹å‡ºé”™: {e}")
        # é™çº§å¤„ç†ï¼šç®€å•åˆ†å‰²
        return simple_fallback_split(content)

def simple_fallback_split(content):
    """é™çº§åˆ†å‰²æ–¹æ³•"""
    paragraphs = content.split('\n\n')
    return [p.strip() for p in paragraphs if p.strip()]
```

## æ€§èƒ½ç‰¹ç‚¹

- **å¤„ç†é€Ÿåº¦**ï¼š1MBæ–‡æ¡£çº¦1-2ç§’
- **å†…å­˜å ç”¨**ï¼šçº¦ä¸ºæ–‡ä»¶å¤§å°çš„2-3å€  
- **å‡†ç¡®ç‡**ï¼šåˆ†æ®µè¾¹ç•Œå‡†ç¡®ç‡>95%
- **ç¨³å®šæ€§**ï¼šæ”¯æŒå¤æ‚Markdownç»“æ„
- **å…¼å®¹æ€§**ï¼šä¸LlamaIndexç®—æ³•100%ä¸€è‡´

## æµ‹è¯•éªŒè¯

### å¿«é€ŸåŠŸèƒ½æµ‹è¯•

```python
def run_quick_test():
    """å¿«é€ŸéªŒè¯åŠŸèƒ½æ˜¯å¦æ­£å¸¸"""
    
    test_content = """
# æµ‹è¯•æ–‡æ¡£

è¿™æ˜¯æµ‹è¯•æ–‡æ¡£çš„ä»‹ç»ã€‚

## ç¬¬ä¸€ç«  åŸºç¡€æ¦‚å¿µ

ä»‹ç»åŸºç¡€æ¦‚å¿µå’ŒåŸç†ã€‚

### 1.1 æ ¸å¿ƒæ¦‚å¿µ

è¯¦ç»†è¯´æ˜æ ¸å¿ƒæ¦‚å¿µã€‚

```python
# æµ‹è¯•ä»£ç å—ä¿æŠ¤
def test_function():
    return "ä»£ç å—åº”è¯¥ä¿æŒå®Œæ•´"
```

### 1.2 åº”ç”¨å®ä¾‹

å®é™…åº”ç”¨çš„ä¾‹å­ã€‚

## ç¬¬äºŒç«  é«˜çº§è¯é¢˜

æ·±å…¥è®¨è®ºé«˜çº§è¯é¢˜ã€‚
"""
    
    try:
        splitter = MarkdownSplitter(max_tokens=500, min_tokens=100)
        chunks = splitter.create_chunks(test_content)
        
        print(f"âœ… æµ‹è¯•é€šè¿‡ï¼šç”Ÿæˆ {len(chunks)} ä¸ªåˆ†æ®µ")
        
        for chunk in chunks:
            print(f"  - {chunk.title}: {chunk.estimated_tokens} tokens")
            
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    run_quick_test()
```

## æ€»ç»“

Markdownæ™ºèƒ½åˆ†æ®µå·¥å…·æä¾›äº†ä¸LlamaIndexå®Œå…¨å…¼å®¹çš„æ ¸å¿ƒç®—æ³•ï¼ŒåŒæ—¶å¢åŠ äº†Tokenæ§åˆ¶ã€è¡¨æ ¼ä¿æŠ¤ç­‰å®ç”¨åŠŸèƒ½ã€‚æ— è®ºæ˜¯ç”¨äºå¤§æ¨¡å‹æ•°æ®é¢„å¤„ç†ã€RAGç³»ç»Ÿä¼˜åŒ–ï¼Œè¿˜æ˜¯æ‰¹é‡æ–‡æ¡£å¤„ç†ï¼Œéƒ½èƒ½æä¾›å¯é ã€é«˜æ•ˆçš„åˆ†æ®µæœåŠ¡ã€‚

é€šè¿‡åˆç†é…ç½®å‚æ•°å¹¶éµå¾ªæœ€ä½³å®è·µï¼Œæ‚¨å¯ä»¥è·å¾—æœ€ä½³çš„åˆ†æ®µæ•ˆæœï¼Œä¸ºAIåº”ç”¨æ‰“ä¸‹åšå®åŸºç¡€ã€‚ 